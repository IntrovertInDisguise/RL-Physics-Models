{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spring mass system\n",
    "Maximize the mass compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "ParseXML: Error opening file '../MujocoEnvs/springmass.xml': No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 175\u001b[0m\n\u001b[0;32m    172\u001b[0m     test_mujoco()\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 175\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 171\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m--> 171\u001b[0m     \u001b[43mtrain_mujoco\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    172\u001b[0m     test_mujoco()\n",
      "Cell \u001b[1;32mIn[13], line 47\u001b[0m, in \u001b[0;36mtrain_mujoco\u001b[1;34m()\u001b[0m\n\u001b[0;32m     45\u001b[0m task_values \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     46\u001b[0m state_histories \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m0\u001b[39m: [], \u001b[38;5;241m1\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msecond_last\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlast\u001b[39m\u001b[38;5;124m'\u001b[39m: []}  \u001b[38;5;66;03m# For specific episodes\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m model, data, renderer \u001b[38;5;241m=\u001b[39m \u001b[43minit_mujoco_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(episodes), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     50\u001b[0m     state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(state_size)  \u001b[38;5;66;03m# Initialize state randomly\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 30\u001b[0m, in \u001b[0;36minit_mujoco_env\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minit_mujoco_env\u001b[39m():\n\u001b[1;32m---> 30\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmujoco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMjModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_xml_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../MujocoEnvs/springmass.xml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     data \u001b[38;5;241m=\u001b[39m mujoco\u001b[38;5;241m.\u001b[39mMjData(model)\n\u001b[0;32m     32\u001b[0m     renderer \u001b[38;5;241m=\u001b[39m mujoco\u001b[38;5;241m.\u001b[39mRenderer(model)\n",
      "\u001b[1;31mValueError\u001b[0m: ParseXML: Error opening file '../MujocoEnvs/springmass.xml': No such file or directory"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import numpy as np\n",
    "import mediapy as media\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import mujoco\n",
    "import time\n",
    "\n",
    "# Q-learning parameters\n",
    "learning_rate = 0.002\n",
    "gamma = 0.99  # Discount factor\n",
    "epsilon = 1.0  # Epsilon-greedy strategy (initial)\n",
    "epsilon_decay = 0.995\n",
    "min_epsilon = 0.01\n",
    "episodes = 10000\n",
    "episode_length = 10  # in seconds\n",
    "timestep = 0.01  # in seconds\n",
    "max_steps_per_episode = int(episode_length / timestep)\n",
    "\n",
    "# Environment setup\n",
    "actions = [-10, 0, +10]  # Force actions\n",
    "action_size = len(actions)\n",
    "\n",
    "# Initialize Q-table uniformly\n",
    "state_size = 10  # Adjust based on the actual state space\n",
    "Q_table = np.random.uniform(low=-1, high=1, size=(state_size, action_size))\n",
    "\n",
    "# MuJoCo environment initialization\n",
    "def init_mujoco_env():\n",
    "    model = mujoco.MjModel.from_xml_path('../MujocoEnvs/springmass.xml')\n",
    "    data = mujoco.MjData(model)\n",
    "    renderer = mujoco.Renderer(model)\n",
    "    return model, data, renderer\n",
    "\n",
    "# Epsilon-greedy action selection\n",
    "def choose_action(state):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(action_size)  # Explore\n",
    "    return np.argmax(Q_table[state])  # Exploit\n",
    "\n",
    "# Training function\n",
    "def train_mujoco():\n",
    "    global epsilon  # Use the global epsilon\n",
    "    rewards = []\n",
    "    task_values = []\n",
    "    state_histories = {0: [], 1: [], 'second_last': [], 'last': []}  # For specific episodes\n",
    "    model, data, renderer = init_mujoco_env()\n",
    "\n",
    "    for episode in tqdm(range(episodes), desc=\"Training\"):\n",
    "        state = np.random.randint(state_size)  # Initialize state randomly\n",
    "        total_reward = 0\n",
    "        task_value = 0\n",
    "        \n",
    "        for step in range(max_steps_per_episode):\n",
    "            action_idx = choose_action(state)\n",
    "            action = actions[action_idx]\n",
    "            next_state, reward, done = step_environment(data, action, timestep)  # Simulate step\n",
    "            \n",
    "            # TD Update\n",
    "            Q_table[state, action_idx] += learning_rate * (reward + gamma * np.max(Q_table[next_state]) - Q_table[state, action_idx])\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            task_value += calculate_task_variable(state)  # Compression or task variable\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Store episode results\n",
    "        rewards.append(total_reward)\n",
    "        task_values.append(task_value)\n",
    "\n",
    "        # Store state history for specific episodes\n",
    "        if episode == 0 or episode == 1 or episode == episodes - 2 or episode == episodes - 1:\n",
    "            state_histories[episode if episode < 2 else ('second_last' if episode == episodes - 2 else 'last')] = get_state_history(data)\n",
    "        \n",
    "        # Epsilon decay\n",
    "        epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "    \n",
    "    # Render episodes\n",
    "    render_episodes(model, state_histories, renderer)\n",
    "\n",
    "    # Plot results\n",
    "    plot_training_results(state_histories, rewards, task_values)\n",
    "\n",
    "def step_environment(data, action, timestep):\n",
    "    data.ctrl[0] = action  # Apply action to the actuator\n",
    "    mujoco.mj_step(data, timestep)\n",
    "    \n",
    "    # Extract relevant state information (e.g., mass position)\n",
    "    state = np.clip(int(data.qpos[0] * 10), 0, state_size - 1)  # Quantize position to a state index\n",
    "    reward = -abs(data.qpos[0])  # Reward function based on compression (maximize compression)\n",
    "    done = False  # Define terminal condition if required\n",
    "    return state, reward, done\n",
    "\n",
    "def calculate_task_variable(state):\n",
    "    # Use compression as the task variable\n",
    "    return np.random.randn()  # Replace with actual task variable\n",
    "\n",
    "def get_state_history(data):\n",
    "    # Collect state history during simulation\n",
    "    return np.random.randn(max_steps_per_episode, state_size)\n",
    "\n",
    "# Render specific episodes\n",
    "def render_episodes(model, state_histories, renderer):\n",
    "    cam = mujoco.MjvCamera()\n",
    "    cam.type = mujoco.mjtCamera.mjCAMERA_FIXED\n",
    "    cam.fixedcamid = model.camera_name2id('cam1')  # Use your camera name\n",
    "    \n",
    "    episode_indices = [0, 1, 'second_last', 'last']\n",
    "    frames = []\n",
    "    \n",
    "    for episode in episode_indices:\n",
    "        state_history = state_histories[episode]\n",
    "        for step in range(max_steps_per_episode):\n",
    "            renderer.update_scene(data, cam, mujoco.MjvOption())\n",
    "            frame = renderer.render()\n",
    "            timestamp = f\"Episode {episode}, Time {step * timestep:.2f}s\"\n",
    "            media.add_text(frame, timestamp, position=(10, 20))  # Adding timestamp to video frame\n",
    "            frames.append(frame)\n",
    "\n",
    "    # Show video with timestamps\n",
    "    media.show_video(frames, fps=int(1/timestep))\n",
    "\n",
    "# Plotting training results\n",
    "def plot_training_results(state_histories, rewards, task_values):\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(10, 10))\n",
    "\n",
    "    # Plot state variables for specific episodes\n",
    "    episode_indices = [0, 1, 'second_last', 'last']\n",
    "    for i, episode in enumerate(episode_indices):\n",
    "        axs[0].plot(state_histories[episode], label=f'Episode {episode}')\n",
    "    axs[0].set_title('State Variables Over Episodes')\n",
    "    axs[0].legend()\n",
    "\n",
    "    # Plot cumulative rewards\n",
    "    axs[1].plot(rewards)\n",
    "    axs[1].set_title('Cumulative Rewards per Episode')\n",
    "    axs[1].set_xlabel('Episode')\n",
    "    axs[1].set_ylabel('Cumulative Reward')\n",
    "\n",
    "    # Plot task variable values\n",
    "    axs[2].plot(task_values)\n",
    "    axs[2].set_title('Task Variable per Episode')\n",
    "    axs[2].set_xlabel('Episode')\n",
    "    axs[2].set_ylabel('Task Variable Value')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Testing function\n",
    "def test_mujoco():\n",
    "    model, data, renderer = init_mujoco_env()\n",
    "    state = np.random.randint(state_size)  # Initialize state randomly\n",
    "\n",
    "    frames = []\n",
    "    for step in range(max_steps_per_episode):\n",
    "        action_idx = np.argmax(Q_table[state])\n",
    "        action = actions[action_idx]\n",
    "        state, reward, done = step_environment(data, action, timestep)\n",
    "        renderer.update_scene(data, cam, mujoco.MjvOption())\n",
    "        frame = renderer.render()\n",
    "        frames.append(frame)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    media.show_video(frames, fps=int(1/timestep))\n",
    "\n",
    "# Main function to run training and testing\n",
    "def main():\n",
    "    train_mujoco()\n",
    "    test_mujoco()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
